\chapter{Evaluierung}

\section{Test}
Die Verifikation der DSL und ihrer Komponenten wird auf zwei Arten durchgeführt: Zum einen mit automatisierten Modultests und zum anderen mit manuellen Integrationstests. Erstere zielen darauf ab, die Funktionsweise von Editor, Transformator und der Modell-Validierung einzeln zu testen und zu verifizieren, dass diese ihre Anforderungen erfüllen. Letztere sollen das Zusammenspiel aller Module der DSL prüfen, indem der Erstellungsprozess für ein Konversationsrouting von der Modellierung bis zur Ausführung nachvollzogen wird.

\subsection{Automatisierte Tests}
Für die automatisierten Tests der DSL-Komponenten kommen Modultests, auch Unit Tests genannt, zum Einsatz. Befürworter der Unit Testing-Praktik befürworten an dieser Technik neben der erhöhten Code-Qualität unter anderem die frühe Ermittlung von Fehlerzuständen (vgl. \cite{Novoseltseva:17}). In der vorliegenden Ausarbeitung wird das Unit Test-Verfahren mit Hilfe des .NET-Test-Frameworks XUnit umgesetzt. Dabei werden Methoden mit einem Attribut als Test-Methoden gekennzeichnet, welche dann von XUnit ausgeführt und ausgewertet werden. Die Testmethoden folgen dem Arrange-Act-Assert-Entwurfsmuster. Nach diesem Muster ist eine Methode in drei Phasen aufgeteilt: In der Arrange-Phase werden die für den Test nötigen Vorbereitungen wie Objekt-Initialisierungen etc. durchgeführt. Anschließend folgt die Act-Phase, in der die zu testende Aktion durchgeführt wird. In der Assert-Phase wird das Testergebnis mit Assertions überprüft. Schlägt keine der Assertions fehl, gilt ein Testfall als bestanden.

\subsubsection{Editor}
In der Editor-Komponente werden zwei Klassen getestet: \texttt{FlowDiagramViewModel} und \texttt{FlowDiagramControl}. Die beiden Komponenten sind vor allem über die Datenbindung des Devexpress-Frameworks miteinander verbunden. Da die Datenbindung jedoch eine Funktion des Devexpress-Frameworks ist und sich das Testen dieser außerhalb des Zuständigkeitsbereichs der vorliegenden Ausarbeitung befindet, werden die beiden Klassen unabhängig voneinander getestet. Für \texttt{FlowDiagramViewModel} ist das Verhalten bei Veränderungen in der Model-Schicht interessant, vor allem wenn es um das Hinzufügen oder Entfernen von Verbindungen geht. Folgende Aspekte der Klasse \texttt{FlowDiagramViewModel} werden unter anderem in den vorliegenden Unit Tests verifiziert: 

\begin{itemize}
\item Das Speichern von \texttt{Flow}-Instanzen im Dateisystem funktioniert
\item Das Laden von \texttt{Flow}-Instanzen aus dem Dateisystem funktioniert
\item Beim Hinzufügen von \texttt{Connector}-Instanzen werden die zugehörigen \texttt{Input}-Referenzen in betroffenen Outputs korrekt gesetzt
\item Beim Entfernen von \texttt{Connector}-Instanzen werden die zugehörigen \texttt{Input}-Re\-fe\-ren\-zen in betroffenen \texttt{Output}-Referenzen auf null gesetzt
\item Beim Setzen einer \texttt{Input}-Referenz in einer \texttt{Output}-Instanz wird eine entsprechende \texttt{Connector}-Instanz hinzugefügt
\item Beim Setzen der \texttt{Input}-Referenz in einer \texttt{Output}-Instanz auf null wird die entsprechende \texttt{Connector}-Instanz entfernt
\item Beim Verändern einer \texttt{Input}-Referenz in einem Output auf eine andere \texttt{Input}-Instanz wird die entsprechende \texttt{Connector}-Instanz aktualisiert 

\end{itemize}

\noindent Auch bei der Klasse \texttt{FlowDiagramControl} fokussieren sich die Testfälle auf das Verhalten im Fall einer Veränderung der Daten, die per Datenbindung zur Verfügung stehen. Die Hauptaufgabe von \texttt{FlowDiagramControl} ist es, diese Daten auf Instanzen der Klasse \texttt{FlowDiagramItem} abzubilden. Da die eigentliche Darstellung der \texttt{FlowDiagramItem}-Instanzen von der Basisklasse und damit vom Devexpress-Framework übernommen wird, wird dies nicht getestet. Stattdessen prüfen die Testfälle, ob es für jede \texttt{Connector}- und jede \texttt{Node}-Subtyp-Instanz eine entsprechende \texttt{DiagramItem}-Instanz gibt. Die Testfälle verifizieren daher folgende Anforderungen:

\begin{itemize}
\item Das Hinzufügen einer \texttt{Node}-Subtyp-Instanz fügt dem Diagramm eine entsprechende \texttt{Flow\-Dia\-gram\-Item}-In\-stanz hinzu
\item Das Entfernen einer \texttt{Node}-Subtyp-Instanz entfernt auch die entsprechende \texttt{Flow\-Dia\-gram\-Item}-In\-stanz
\item Das Hinzufügen einer \texttt{FlowDiagramItem}-Instanz fügt eine entsprechende \texttt{Node}-Subtyp-Instanz hinzu
\item Das Entfernen einer \texttt{FlowDiagramItem}-Instanz entfernt eine entsprechende \texttt{Node}-Subtyp-Instanz
\item Das Hinzufügen einer \texttt{Connector}-Instanz fügt dem Diagramm eine entsprechende \texttt{Flow\-Dia\-gram\-Con\-nec\-tor}-In\-stanz hinzu
\item Das Entfernen einer \texttt{Connector}-Instanz entfernt auch die entsprechende \texttt{Flow\-Dia\-gram\-Con\-nec\-tor}-In\-stanz
\item Das Hinzufügen einer \texttt{FlowDiagramConnector}-Instanz fügt auch eine entsprechende \texttt{Connector}-Instanz hinzu
\item Das Entfernen einer \texttt{FlowDiagramConnector}-Instanz entfernt auch die entsprechende \texttt{Connector}-Instanz
\item Das Ändern der Endpunkte einer \texttt{FlowDiagramConnector}-Instanz passt auch die entsprechende \texttt{Connector}-Instanz an
\end{itemize} 

\subsubsection{Transformator}  
In den Tests für die Transformator-Komponenten werden die wichtigsten Klassen für die Bytecode-Generierung verifiziert. Dabei ist der Zuständigkeitsbereich einer Klasse zu beachten, und dementsprechend zu testen. Es macht zum Beispiel wenig Sinn, die Details der Syntax der generierten Klassen in den Testfällen für \texttt{ConversationRoutingBehaviorGenerator} zu verifizieren, da sich diese Klasse mit der Kompilierung der Syntax und nicht deren Zusammensetzung beschäftigt. Die Syntax der generierten Klassen wird in den Modulen \texttt{FlowClassSyntaxBuilder} und \texttt{UserCodeClassSyntaxBuilder} erstellt und sollte dementsprechend in den dort zugehörigen Testfällen verifiziert werden. Auf diese Weise wird eine unnötige mehrfache Testüberdeckung vermieden. Vor diesem Hintergrund werden für die Klasse \texttt{ConversationRoutingBehaviorGenerator} folgende Aspekte mittels Unit Tests überprüft: 

\begin{itemize}
\item Im Falle einer übergebenen korrekten \texttt{Flow}-Instanz wird die C\#-Syntax erfolgreich erstellt, kompiliert und es wird eine In-Memory-Assembly zurückgegeben, in der die erwartete Konversationsrouting-Klasse definiert ist
\item Im Falle einer übergebenen \texttt{Flow}-Instanz, in der keine Start-Instruktion vorhanden ist, wird eine Exception geworfen
\item Im Falle einer fehlgeschlagenen Kompilierung der generierten Syntax wird eine Exception geworfen, in der detaillierte Fehlermeldungen enthalten sind
\item Im Falle einer übergebenen korrekten \texttt{Flow}-Instanz wird von Roslyn ein semantisches Modell der C\#-Syntax zurückgegeben
\end{itemize} 

\noindent Die nächstniedrigere Stufe bei der Transformation ist die Erstellung des C\#-Syntaxbaums. Hierfür wird die Klasse \texttt{ConversationRoutingSyntaxTreeBuilder} folgendermaßen getestet:

\begin{itemize}
\item Im Falle einer übergebenen Start-Instruktion wird erfolgreich ein C\#-Syn\-tax\-baum generiert, welcher auf der höchsten Ebene die Syntax für die erwartete Klassendeklarationen im erwarteten Namespace enthält
\item Im Falle von ungenügenden übergebenen Parametern wird eine Exception geworfen
\end{itemize}

\noindent Der Syntaxbaum besteht zum größten Teil aus der Deklaration der Klasse, die das Konversationsrouting umsetzt. Die Syntax dieser Klasse wird von \texttt{Flow\-Class\-Syn\-tax\-Buil\-der} erstellt und im Zuge der zugehörigen Testfälle verifiziert:

\begin{itemize}
\item Im Falle einer übergebenen Start-Instruktion eines korrekten DSL-Modells in Form einer \texttt{StartNode}-Instanz wird die Syntax für eine Klasse zurückgegeben, die von \texttt{ACDCallRoutingBehaviorBase} erbt, und für jede im Modell enthaltene Instruktion eine Membermethode beinhaltet. Zusätzlich existiert eine geschachtelte private Klassendeklaration für den benutzerspezifischen Code, welche auch als Membervariable in der umfassenden Klasse vertreten ist
\item Im Falle von fehlerhaften übergebenen Parametern wird eine Exception geworfen.
\end{itemize}

\noindent \texttt{UserCodeClassSyntaxBuilder}, welche für die Generierung der geschachtelten Klassendeklaration zuständig ist, wird auf folgende Anforderungen getestet:

\begin{itemize}
\item Im Falle einer übergebenen Start-Instruktion eines korrekten DSL-Modells in Form einer \texttt{StartNode}-Instanz wird die Syntax für eine Klasse zurückgegeben, in der alle Instruktionen des Modells, die benutzerspezifischen Code enthalten, mit einer Membermethode vertreten sind. Diese Membermethoden führen den vom Benutzer geschriebenen Code aus. Zusätzlich existieren für benutzerdefinierte  Funktionen entsprechende Membermethoden und für benutzerdefinierte Variablen entsprechende Membervariablen. Die Klasse enthält weiterhin zwei Properties vom Typ String mit den Namen \texttt{Skill} und \texttt{Language}.
\item Im Falle von fehlerhaften übergebenen Parametern wird eine Exception geworfen.
\end{itemize}

\noindent Ob die Syntax für generierte Methoden, die den Ablauf des Routings steuern, die richtige Struktur aufweist, wird in den Testfällen für die Klasse \texttt{Meth\-od\-Flow\-Syn\-tax\-Buil\-der} wie folgt getestet:

\begin{itemize}
\item Wird eine Instruktion in Form einer \texttt{Node}-Subtyp-Instanz übergeben, wird eine Liste an Methodendeklarationen zusammengestellt, in der alle von der übergebenen \texttt{Node}-Subtyp-Instanz erreichbaren Instruktionen abgebildet sind. In der Syntax für jeden Methodenkörper existieren die Methodenaufrufe für die im Modell jeweils nachfolgende Instruktion.
\item Werden unzureichende Parameter übergeben, wird eine Exception geworfen
\end{itemize}

\noindent Die Methoden-Deklarationen für die Klasse \texttt{UserCode} werden von \texttt{User\-Code\-Meth\-od\-Syn\-tax\-Buil\-der} umgesetzt und folgendermaßen getestet:

\begin{itemize}
\item Wird eine Instruktion in Form einer \texttt{Node}-Subtyp-Instanz übergeben, wird eine Liste an Methodendeklarationen zusammengestellt, in der nur die Instruktionen abgebildet sind, in denen vom Benutzer geschriebener Code zum Einsatz kommt.
\item Werden unzureichende Parameter übergeben, wird eine Exception geworfen
\end{itemize}

\noindent Zuletzt wird noch getestet, ob der Inhalt der generierten Methoden den Anforderungen entspricht. Dafür werden alle von \texttt{NodeSyntaxBuilder} abgeleiteten Typen getestet, da diese die instruktionsspezifische Syntax  für eine Methode generieren:

\begin{itemize}
\item Jeder Subtyp von \texttt{NodeSyntaxBuilder} erstellt für die ihm übergebene Instanz des zugehörigen \texttt{Node}-Subtyps die spezifische Syntax, die notwendig ist, um diese Instruktion umzusetzen. Die Syntax wird in Form einer Liste von \texttt{SyntaxNode}-Instanzen zurückgeliefert 
\item Werden unzureichende Parameter übergeben, wie zum Beispiel eine Instanz eines falschen \texttt{Node}-Subtyps, wird eine Exception geworfen
\end{itemize}

\subsubsection{Modell-Validierung}  
Die Modell-Validierung stellt eine eigene Komponente dar und wird daher auch in einer separaten Sammlung an Unit-Tests verifiziert. Im Fokus stehen vor allem die ConstraintEvaluators, welche auf ihre Funktionstüchtigkeit geprüft werden. Im Detail werden folgende Tests durchgeführt:

\begin{itemize}
\item Wird ein DSL-Modell ohne Start-Instruktion validiert, gibt \texttt{Start\-Node\-Ex\-ists\-Eva\-lua\-tor} eine entsprechende Diagnostik aus
\item Wird ein DSL-Modell mit mehr als einer Start-Instruktion validiert, gibt \texttt{Start\-Node\-Ex\-ists\-E\-val\-u\-a\-tor} eine entsprechende Diagnostik zurück
\item Beinhaltet ein DSL-Modell eine Start-Instruktion, gibt \texttt{Start\-Node\-Ex\-ists\-Eva\-lua\-tor} keine Diagnostik zurück
\item Für jede unverbundene Instruktion in einem DSL-Modell gibt \texttt{No\-Un\-con\-nect\-ed\-Node\-Ex\-ists\-Eva\-lua\-tor} eine Diagnostik zurück, die auf die entsprechende Instruktion verweist
\item Für jeden ungültigen Instruktionsparameter gibt \texttt{Node\-Pa\-ra\-me\-ters\-Are\-Val\-id\-Eval\-u\-a\-tor} eine Diagnostik mit Referenz auf die betroffene Instruktion zurück
\item Für jeden Compiler-Fehler in benutzerdefiniertem Code gibt \texttt{U\-ser\-Code\-Com\-piles\-E\-val\-u\-a\-tor} eine Diagnostik mit einer Referenz auf die betroffene Instruktion oder Funktion zurück
\item Jeder von \texttt{U\-ser\-Code\-Com\-piles\-E\-val\-u\-a\-tor} bemängelte Compiler-Fehler besitzt eine korrekte Stellenangabe, die anzeigt, wo im Benutzer-Code der Fehler liegt
\end{itemize} 

\noindent Bei allen Tests, die als Parameter eine Flow-Instanz entgegennehmen, wird das Modell aus Abb. \ref{fig:TestRouting} verwendet. 

\subsection{Manuelle Tests}
Bei den manuellen Tests geht es darum, das korrekte Zusammenwirken der einzelnen Komponenten zu testen. Daher werden alle Schritte, die für die Inbetriebnahme eines Konversationsroutings von Nöten sind, ausgeführt und die korrekte Funktionsweise anschließend verifiziert. Zu diesem Zweck werden folgende Schritte in der beschriebenen Reihenfolge ausgeführt:
\begin{description}
\item[Modellierung] \hfill \\
Mittels des Editors wird ein Modell angefertigt und abgespeichert. In dem Modell sind alle Arten von Instruktionen enthalten, welche auch alle mindestens einmal über einen Pfad des Konversationsroutings ausgeführt werden. Das Routing muss mindestens einen Zyklus in seinem Ablauf vorweisen. Zusätzlich sind mindestens je eine benutzerdefinierte Variable und Funktion im Routing enthalten, die auch in einer oder mehreren Script-Instruktionen verwendet werden. Bei der Modellierung des Routings müssen alle Editoren (Script-, Listen-, Ausdrucks- und Routing-Editor) mindestens einmal ausgeführt werden. Nach dem Editieren eines jeden Wertes muss durch erneutes Editieren überprüft werden, ob der Wert auch tatsächlich übernommen wurde. Treten während des Modellierens keine Fehler auf, wird das Routing im Dateisystem abgespeichert. 
\item[Ausführung] \hfill \\
Als nächstes wird die Routing Engine gestartet. Diese ist so konfiguriert, dass sie beim Start das im vorherigen Schritt modellierte Routing lädt und bei einem eingehenden Anruf ausführt.
\item[Anruf] \hfill \\
Nachdem die Routing Engine gestartet ist, kann das Modell über Anrufe getestet werden. Es werden so viele Anrufe getätigt, dass jeder Pfad des Routings einmal ausgeführt wurde und sich das Verhalten mit der Spezifikation des Modells deckt. Insbesondere wird stichprobenartig für jeden Pfad getestet, wie sich das System verhält, wenn entweder der Anrufer oder ein beteiligter Agent frühzeitig den Anruf beendet. In diesen Fällen dürfen für ein Bestehen des Tests keine Anrufe auf Agenten- oder Kundenseite unbeantwortet übrigbleiben.
\end{description}
Ein Kandidat für ein Test-Routing, mit dem die obenstehenden Schritte ausgeführt werden, ist in Abbildung \ref{fig:TestRouting} zu sehen.

\begin{figure} %[hbtp]
	\centering
		\includegraphics[width=\textwidth]{img/TestRouting.png}
	\caption[DSL-Modell für Tests]{\textit{Zu sehen ist das DSL-Modell, welches im Zuge von automatisierten und manuellen  Tests zum Einsatz kommt.}}
	\label{fig:TestRouting}
\end{figure}
\noindent Neben dem Kriterium, alle Instruktionen zu beinhalten, bietet es eine überschaubare Anzahl an Pfaden, die durch das Konversationsrouting  führen und kann so mit wenigen Anrufen ganz abgedeckt werden. Auf der Abbildung sind die Skripte sowie die benutzerdefinierten Variablen und Funktionen, die zum Einsatz kommen, nicht zu sehen. Angelegt ist die Integer-Variable \texttt{Count}, welche in der Set Variables-Instruktion mit null (in Ziffern: 0) initialisiert wird. Anschließend wird in der Branch-Instruktion abgefragt, ob \texttt{Count} größer als zehn ist und ob es sich um eine gerade Zahl handelt. Letzteres wird mit einer benutzerdefinierten Funktion \texttt{IsEven} überprüft. Die Branch-Instruktion nimmt also zuerst den False-Ausgang, welcher in das erste Skript führt. Dort wird die \texttt{Count}-Variable um eins inkrementiert und führt zurück in die Branch-Instruktion. Diese Schleife wird solange ausgeführt, bis die Branch-Instruktion den True-Ausgang nimmt. Dieser führt zum zweiten Skript, in der die benutzerdefinierte Funktion \texttt{PrintTime} aufgerufen wird, welche die aktuelle Systemzeit auf der Konsole ausgibt.

\section{Metriken des geschriebenen Codes}
Bei der Umsetzung der DSL entstehen zwei Arten von Code. Die eine Art von Quellcode setzt das System um: Es handelt sich hierbei um den Quellcode, der den Editor, den Transformator und die Modell-Validierung implementiert. Die andere Art von Code ist der vom System generierte Code, also der CIL-Bytecode, der ein Konversationsrouting umsetzt. Die erste Code-Art ist für den weiteren Lebenszyklus der DSL von unmittelbarer Bedeutung und muss daher vor allem wartbar sein. Daher folgt eine Einschätzung zum vermessenen Umfang und Komplexität dieses Code-Anteils. Diese Messungen für den generierten Byte-Code durchzuführen, wäre  wenig aussagekräftig, da sowohl Umfang als auch Komplexität stark vom transformierten Modell abhängt: Je komplexer und umfangreicher das Konversationsrouting, desto komplexer ist der Code, der dieses Konversationsrouting umsetzt. Die Grenzen sind vor allem nach oben nicht abschätzbar, da der Benutzer in Script-Instruktionen und eigenen Funktionen ``unvorhersehbaren'' Code einfügen kann. Da außerdem aus der erstellten Syntax für Benutzer unleserlicher Bytecode generiert wird, welcher nicht dafür gedacht ist per Hand gewartet zu werden, ist das Berechnen von detaillierten Metriken für den so erstellten Code wenig zielführend. 
\newline
Berechnet wurden die folgenden Metriken mit der Entwicklungsumgebung Visual Studio 2017 Ultimate. Zum Einsatz kommen die ``executable lines of code'', bei der die ausführbaren Bytecode-Zeilen gemessen werden, um einen generellen Überblick über den Umfang der Code-Basis zu liefern, und die zyklomatische Komplexität, auch als McCabe-Metrik bekannt, welche den Grad an Verzweigungen im Programmfluss beschreibt. Beide Metriken sind im Feld der Software-Architektur verbreitete Maße, deren Ermittlung jeweils bei \cite[S. 35ff]{Laird:06} und \cite[S. 58ff]{Laird:06} nachgelesen werden kann. Da die Angaben für gesamte Namespaces gemacht werden, wird bei der zyklomatischen Komplexität der Mittelwert und der Median über alle Methoden dieses Namespaces angegeben. Alle der genannten Namespaces sind Unter-Namespaces von \texttt{ilogixx.ConversationFlow}. In Tabelle \ref{tab:metrikenGeschriebenerCode} sind die gemessenen Metriken für den im Zuge der vorliegenden Arbeit händisch verfassten Code zu sehen.

\begin{table}[hbtp]
\centering
\settowidth\tymin{\textbf{Komponente}}
\begin{tabulary}{1.0\textwidth}{L|L|C|C|C}
\textbf{Komponente} & \textbf{Namespace} & \textbf{LOC} & \textbf{zykl. Kompl. (mittel)} & \textbf{zykl Kompl. (median)} \\ 
\hline
Objekt-Graph & Core & 183 & 1,146341463 & 1 \\ 
\hline
Editor & UI & 2818 & 2,212669683 & 1 \\ 
\hline
Transformator & Generation & 742 & 1,715846995 & 1 \\ 
\hline
Validator & Validation & 186 & 1,806451613 & 1 \\ 
\end{tabulary}
\caption{\textit{Code-Metriken des verfassten Quellcodes.}}
\label{tab:metrikenGeschriebenerCode}
\end{table}
Die Klassenstruktur des Objekt-Graphen weist mit der geringsten Anzahl an Lines of Code und der kleinsten durchschnittlichen zyklomatischen Komplexität die niedrigste Komplexität auf. Dies ist nicht überraschend, da es sich um eine fast reine Datenstruktur handelt, die wenig programmatisches Verhalten aufweist. An zweiter Stelle kommt die Komponente der Modell-Validierung. Komplexe Abläufe und ein hoher Umfang an Code konnte hier vermieden werden, da die meiste Arbeit, welche die Metriken erhöhen könnte, an andere Komponenten ausgelagert wird, wie zum Beispiel den Transformator. Dieser weist eine höhere Verzweigung des Programmablaufs und einen größeren Umfang auf, wie die Metriken zum Ausdruck bringen. Dennoch bleibt auch hier die durchschnittliche McCabe-Metrik deutlich unter dem empfohlenen Wert von zehn, auch dank der rekursiven Vorgehensweise dieser Komponente. Durch diese werden Schleifen vermieden, welche einen hohen Einfluss auf die Anzahl an mögliche Programmpfaden haben. Der hohe Anteil an Lines of Code für den Transformator lässt sich durch den Gebrauch der Roslyn-API begründen, denn im Zuge des Aufbaus eines kompletten C\#-Syntaxbaums werden im Vergleich zu anderen Komponenten  viele API-Aufrufe benötigt. 
\newline
Die laut den obenstehenden Metriken komplexeste Komponente der DSL ist der Editor. Der im Vergleich extrem hohe LOC-Wert für diese Komponente lässt sich damit begründen, dass automatisch generierter Code des Windows Forms-Frameworks in diesem Namespace enthalten ist. Windows Forms benutzt einen sogenannten Component Designer, welcher ``Boilerplate''-Code zum Erstellen von Windows Forms-Komponenten automatisch generiert. Dieser Code ist sehr umfangreich, fällt aber wenig ins Gewicht, da der vom Component Designer generierte Code nicht von Hand lesbar oder editierbar sein soll und damit nicht die Komplexität des Systems widerspiegelt. Der hohe Mittelwert der zyklomatischen Komplexität ist auf die Fallunterscheidungen zurückzuführen, die auf View- und ViewModel-Ebene durchgeführt werden, wenn es um die Behandlung von Events geht, welche durch die Benutzung der grafischen Benutzeroberfläche ausgelöst werden.
\newline
Erwähnenswert ist weiterhin, dass keine Komponente beim Median der zyklomatischen Komplexität einen Wert von eins überschreitet. In Verbindung mit dem geringen Mittelwert aller Komponenten spricht dies für eine hohe Geradlinigkeit des Programmflusses. 

\section{Performanz des geschriebenen Codes}
Sowohl der generierte als auch der händisch verfasste Code muss eine hohe Leistungsfähigkeit aufweisen. Aber ähnlich wie im vorangegangen Kapitel gestaltet sich die Abschätzung der Performanz für den generierten Bytecode als schwierig: Der Code kann nur so effizient sein, wie der Benutzer ihn gestaltet. Zusätzlich haben Maße wie Zeitmessungen wenig Bedeutung, da ein Konversationsrouting an vielen Stellen in erwünschten Maßen blockiert. Eine Laufzeitanalyse ist in diesem Fall ebenfalls wenig zielführend, da die Laufzeit des Algorithmus, der ein Konversationsrouting ausführt, von keiner Eingabe abhängig ist, sondern höchstens von der Interaktion mit einem Anrufer.
\newline
Aus diesem Grund konzentriert sich die folgende Analyse auf den Code, welcher die Transformation durchführt. Auch hier ist eine hohe Performanz wichtig, da dieser Code im Zuge der Modell-Validierung und damit laufend während der Modellierung ausgeführt wird. Leistungsfähiger Code kann hier also dafür sorgen, dass der Benutzer schnelles Feedback über etwaige Fehler bekommt und bei der Modellierung nicht lange auf die Transformation warten muss.

\subsubsection{Zeitmessungen}
Der folgende Versuch soll eine ungefähre Vorstellung davon geben, wie lange die Transformationskomponente für die Generierung einer Assembly benötigt und in welchem Verhältnis diese Dauer zum Umfang eines zu transformierenden DSL-Modells steht. Dafür wurden DSL-Modelle mit einer Anzahl an Instruktionen zwischen 1 und 600 transformiert, während jeweils die Dauer der Syntax-Generierung und der anschließenden Kompilierung durch die Roslyn-API gemessen wurde. Zwischen je zwei Transformations-Versuchen wurde die Anzahl der Instruktionen um eine einzelne Media Playback-Instruktion erhöht, sodass eine eindimensionale wachsende Kette an Instruktionen zu einer Assembly konvertiert wurde. Der Aufbau dieser Konversationsroutings ist zwar nicht repräsentativ für den realen Anwendungsfall, ermöglicht aber den Vergleich zwischen Testergebnissen von Konversationsroutings mit sehr vielen Instruktionen. Die Schritte des Ladens der Datei und der Deserialisierung wurden dabei ausgelassen, da diese in dem Anwendungsszenario der kontinuierlichen Transformation während der Modellierung durch den Benutzer lediglich einmal und nicht wiederholt ausgeführt werden. Für jedes DSL-Modell wurde die durchschnittliche Dauer aus hundert konsekutiven Transformationen gemessen, um das Testergebnis vor zufälligen Abweichungen der Laufzeitumgebung und des Betriebssystems abzuschirmen. Zusätzlich ist die Initialisierungszeit, die die Roslyn-API beim ersten Gebrauch benötigt, nicht in den Messungen enthalten. Alle Messungen wurden auf einem Intel Core i7-4770 mit 3,40 GHz durchgeführt. Die Ergebnisse sind in Abb. \ref{fig:AverageTimeDiagram} zu sehen. 


\begin{figure} %[hbtp]
	\centering
		\includegraphics[width=\textwidth]{img/AverageTimeDiagram2.png}
	\caption[Durchschnittliche Dauer von Assembly-Generierungen]{\textit{Zu sehen ist die durchschnittliche Dauer der Phasen einer Assembly-Generierung in Abhängigkeit der Instruktions-Anzahl eines Konversationsroutings. Der Durchschnitt für eine bestimmte Anzahl an Instruktionen wurde jeweils aus hundert konsekutiven Assembly-Generierungen errechnet.}}
	\label{fig:AverageTimeDiagram}
\end{figure}
\noindent Zu beobachten ist unter anderem ein annähernd linearer Anstieg der Kompilierungsdauer in Abhängigkeit zur Instruktionsanzahl. Bei der Dauer der Syntaxgenerierung hingegen zeichnet sich ein polynomieller Wuchs ab. Diese Annahme wird durch eine Analyse mit dem Bestimmtheitsmaß $R^{2}$ bekräftigt: Dabei handelt es sich um ein Gütemaß zwischen null und eins, was für eine Datenreihe die Entfernung zu einer bestimmten Funktion bewertet (siehe zur Berechnung \cite{Pflieger:14}). Dabei sagen hohe Werte aus, dass eine Funktion nah an einer Datenreihe ist, beziehungsweise die Datenreihe wenig von der Funktion abweicht. Berechnet man den bestmöglichen $R^{2}$-Wert der Datenreihe ``Syntax Generierung'' für Funktionen verschiedener Arten (linear, polynomiell, logarithmisch), so erreicht man die höchsten Werte bei Polynomen ab dem Grad zwei. Die Anteile von Polynomen höheren Grades, die über den linearen Teil hinausgehen, werden dabei zwar klein, aber nicht vernachlässigbar, sodass ein polynomielles Wachstum trotz Messfehler wahrscheinlich erscheint. 
\newline
Bei der Datenreihe der Kompilierungsdauer ist die Entscheidung schwieriger: Zwar werden die besten $R^{2}$-Werte auch mit Polynomen höheren Grades erzielt. Allerdings werden die Faktoren dieser Polynome, die über den linearen Anteil hinaus gehen, sehr klein (der quadratische Faktor bei einem Polynom zweiten Grades ist beispielsweise 0,0005). Außerdem sind die so erzielten $R^{2}$-Werte nicht signifikant besser als für eine rein lineare Funktion. Erklärbar ist dies durch den ``Ausreißer'' der Datenreihe ab einer Instruktionsanzahl von 600: Ab hier erhöht sich die durchschnittliche Dauer um über 100 Millisekunden. Dieser Ausschlag ist vermutlich ein Messfehler bedingt durch äußere Einflüsse der Testumgebung und der Grund, warum eine polynomielle Funktion einen besseren $R^{2}$-Wert erzielt als eine intuitiv naheliegende lineare Funktion.
\newline
Ebenfalls erkennbar ist bei der Messung der Punkt, an dem die Generierung der Syntax länger dauert als die eigentliche Kompilierung: Ab einer Anzahl von ca. 260 Instruktionen ist dies der Fall.  Zusammengenommen dauert der gesamte Vorgang der Assembly-Generierung dann im Schnitt ca. 400 Millisekunden. Anzumerken ist allerdings, dass in einem normalen Anwendungsfall der DSL eine Anzahl von 260 Instruktionen die zu erwartende Instruktionsanzahl eines DSL-Modells weit übersteigt. Ein Konversationsrouting mit derart vielen Instruktionen erreicht ein Level an Komplexität, das durch den Endbenutzer nur noch schwer nachvollziehbar ist. Daher sind Konversationsroutings in einem solchen Umfang unwahrscheinlich.  
